{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "402e97e4",
   "metadata": {},
   "source": [
    "Facial Expression Recognition using Deep Learning\n",
    "By\n",
    "Dhushyanthan K\n",
    "210103040"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0695a379",
   "metadata": {},
   "source": [
    "Motivation: Why did I pick this topic?\n",
    "Facial expressions are a powerful medium of non-verbal communication. In human-computer interaction (HCI), recognizing emotional expressions can help build smarter, more empathetic systems â€” from mental health apps to driver monitoring systems. I chose this topic because it combines deep learning with computer vision to solve a problem that is both challenging and impactful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6e2f6e",
   "metadata": {},
   "source": [
    "Learnings from this Work\n",
    "From this project, I learned:\n",
    "\n",
    "How to preprocess a challenging dataset like FER2013.\n",
    "\n",
    "How to build and train deep CNN models (like mini-XCEPTION).\n",
    "\n",
    "How real-time inference with OpenCV can be integrated with deep learning models.\n",
    "\n",
    "How important preprocessing and data augmentation are for generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f07a45c",
   "metadata": {},
   "source": [
    "Connection to Multimodal Learning (Recent Historical Perspective)\n",
    "Facial expression recognition (FER) is a key component in the broader field of affective computing. Recent work in multimodal learning integrates FER with speech and gesture analysis to understand human emotions better. The 2020s have seen models like CLIP and Flamingo enabling cross-modal understanding, but expression recognition from visual cues remains fundamental. This project contributes to that visual modality by enhancing emotion prediction using CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60e67e3",
   "metadata": {},
   "source": [
    "# Code, Demos, and Visualizations\n",
    "This project has three major stages: Data Preprocessing, Model Training, and Real-time Inference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62543d2a",
   "metadata": {},
   "source": [
    "1. Data Loading and Preprocessing\n",
    "We use the FER-2013 dataset, a standard benchmark for facial expression recognition. It contains 48x48 grayscale images and labels for 7 emotions."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d79b3ccb",
   "metadata": {},
   "source": [
    "# File: load_and_process.py\n",
    "\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Path to dataset\n",
    "dataset_path = 'fer2013/fer2013/fer2013.csv'\n",
    "image_size = (48, 48)\n",
    "\n",
    "def load_fer2013():\n",
    "    data = pd.read_csv(dataset_path)\n",
    "    pixels = data['pixels'].tolist()\n",
    "    faces = []\n",
    "    \n",
    "    for pixel_sequence in pixels:\n",
    "        face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
    "        face = np.asarray(face).reshape(48, 48)\n",
    "        face = cv2.resize(face.astype('uint8'), image_size)\n",
    "        faces.append(face.astype('float32'))\n",
    "\n",
    "    faces = np.expand_dims(np.asarray(faces), -1)\n",
    "    emotions = pd.get_dummies(data['emotion']).values\n",
    "    return faces, emotions\n",
    "\n",
    "def preprocess_input(x, v2=True):\n",
    "    x = x / 255.0\n",
    "    if v2:\n",
    "        x = x - 0.5\n",
    "        x = x * 2.0\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6be203",
   "metadata": {},
   "source": [
    " 2. Model Training\n",
    "We used a modified version of the mini-XCEPTION architecture due to its efficiency and strong performance in emotion recognition tasks. The model was trained using data augmentation to increase generalization."
   ]
  },
  {
   "cell_type": "raw",
   "id": "afc7b975",
   "metadata": {},
   "source": [
    "# File: train_emotion_classifier.py\n",
    "\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from load_and_process import load_fer2013, preprocess_input\n",
    "from models.cnn import mini_XCEPTION  # Custom CNN architecture\n",
    "\n",
    "# Load and preprocess\n",
    "faces, emotions = load_fer2013()\n",
    "faces = preprocess_input(faces)\n",
    "\n",
    "# Train-test split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(faces, emotions, test_size=0.2, shuffle=True)\n",
    "\n",
    "# Data Augmentation\n",
    "data_generator = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "model = mini_XCEPTION((48, 48, 1), 7)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint('models/my_model.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', save_best_only=True),\n",
    "    CSVLogger('models/_emotion_training.log'),\n",
    "    EarlyStopping(monitor='val_loss', patience=50),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=12)\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    data_generator.flow(xtrain, ytrain, batch_size=32),\n",
    "    steps_per_epoch=len(xtrain) // 32,\n",
    "    epochs=10000,\n",
    "    validation_data=(xtest, ytest),\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b15c55f",
   "metadata": {},
   "source": [
    "# 3. Real-Time Emotion Detection\n",
    "Finally, we implemented a real-time video application using OpenCV that:\n",
    "\n",
    "Detects faces using Haar cascades.\n",
    "\n",
    "Classifies the detected face using the trained CNN.\n",
    "\n",
    "Displays the top predicted emotion with a confidence bar."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0fff2714",
   "metadata": {},
   "source": [
    "# File: real_time_video.py\n",
    "\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import imutils\n",
    "import numpy as np\n",
    "\n",
    "# Load Haar cascade and trained model\n",
    "face_detection = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "emotion_classifier = load_model('models/_mini_XCEPTION.102-0.66.hdf5', compile=False)\n",
    "EMOTIONS = [\"angry\", \"disgust\", \"scared\", \"happy\", \"sad\", \"surprised\", \"neutral\"]\n",
    "\n",
    "camera = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    frame = camera.read()[1]\n",
    "    frame = imutils.resize(frame, width=300)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_detection.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "    if len(faces) > 0:\n",
    "        (fX, fY, fW, fH) = sorted(faces, key=lambda x: x[2] * x[3], reverse=True)[0]\n",
    "        roi = gray[fY:fY + fH, fX:fX + fW]\n",
    "        roi = cv2.resize(roi, (64, 64)).astype(\"float\") / 255.0\n",
    "        roi = img_to_array(roi)\n",
    "        roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "        preds = emotion_classifier.predict(roi)[0]\n",
    "        label = EMOTIONS[preds.argmax()]\n",
    "        \n",
    "        # Draw box and label\n",
    "        cv2.putText(frame, label, (fX, fY - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255, 0, 0), 2)\n",
    "        cv2.rectangle(frame, (fX, fY), (fX + fW, fY + fH), (255, 0, 0), 2)\n",
    "\n",
    "    cv2.imshow('Real-time Emotion Detection', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f930a72d",
   "metadata": {},
   "source": [
    "Reflections\n",
    "(a) What surprised me?\n",
    "\n",
    "Even a compact model like mini-XCEPTION can achieve decent real-time accuracy with the right preprocessing and augmentation.\n",
    "\n",
    "Real-time emotion prediction is more sensitive to lighting and facial angles than expected.\n",
    "\n",
    "(b) What can be improved?\n",
    "\n",
    "Integrate audio or body gestures for richer emotion understanding (multimodal aspect).\n",
    "\n",
    "Use more recent architectures (e.g., MobileNetV3 or Vision Transformers).\n",
    "\n",
    "Include attention mechanisms for more robust feature extraction.\n",
    "\n",
    "Handle multiple faces with multi-label output instead of just the largest face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a00dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db47ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
